工作流程
网络爬虫是捜索引擎（Baidu、Google、Yahoo）抓取系统的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。
网络爬虫的基本工作流程如下：
首先选取一部分精心挑选的种子URL；
将这些URL放入待抓取URL队列；
从待抓取URL队列中取出待抓取在URL，解析DNS，并且得到主机的ip，并将URL对应的网页下载下来，存储进已下载网页库中。此外，将这些URL放进已抓取URL队列。
分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环。